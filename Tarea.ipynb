{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prospective-america",
   "metadata": {},
   "source": [
    "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local.\n",
    "\n",
    "Al entrar en [Google Colab](https://colab.research.google.com) bastará con hacer click en `upload` y subir este notebook. No olvide luego descargarlo en `File->Download .ipynb`\n",
    "\n",
    "**El examen deberá ser entregado con las celdas ejecutadas, si alguna celda no está ejecutadas no se contará.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-stewart",
   "metadata": {},
   "source": [
    "El examen se divide en preguntas de código y preguntas teóricas, con la puntuación que se indica a continuación. La puntuación máxima será 10.\n",
    "\n",
    "- [Actividad 1: Redes Densas](#actividad_1): 10 pts\n",
    "    - Correcta normalización: máximo de 0.5 pts\n",
    "    - [Cuestión 1](#1.1): 1.5 pts\n",
    "    - [Cuestión 2](#1.2): 1.5 pts\n",
    "    - [Cuestión 3](#1.3): 1.5 pts\n",
    "    - [Cuestión 4](#1.4): 1 pts\n",
    "    - [Cuestión 5](#1.5): 1 pts\n",
    "    - [Cuestión 6](#1.6): 1 pts\n",
    "    - [Cuestión 7](#1.7): 1 pts\n",
    "    - [Cuestión 8](#1.8): 1 pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-correction",
   "metadata": {},
   "source": [
    "<a name='actividad_1'></a>\n",
    "# Actividad 1: Redes Densas\n",
    "\n",
    "Para esta actividad vamos a utilizar el [wine quality dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality). Con el que trataremos de predecir la calidad del vino.\n",
    "\n",
    "La calidad del vino puede tomar valores decimales (por ejemplo 7.25), independientemente de que en el dataset de entrenamiento sean números enteros. Por lo tanto, el problema es una `regresión`.\n",
    "\n",
    "**Puntuación**: \n",
    "\n",
    "Normalizar las features correctamente (x_train, x_test): 0.5 pts \n",
    "\n",
    "- Correcta normalización: máximo de 0.5 pts\n",
    "- [Cuestión 1](#1.1): 1 pt\n",
    "- [Cuestión 2](#1.2): 1 pt\n",
    "- [Cuestión 3](#1.3): 0.5 pts\n",
    "- [Cuestión 4](#1.4): 0.5 pts\n",
    "- [Cuestión 5](#1.5): 0.5 pts\n",
    "- [Cuestión 6](#1.6): 0.5 pts\n",
    "- [Cuestión 7](#1.7): 0.5 pts\n",
    "- [Cuestión 8](#1.8): 0.5 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "presidential-milan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fixed acidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volatile acidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "citric acid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "residual sugar",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "chlorides",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "free sulfur dioxide",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total sulfur dioxide",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "density",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pH",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sulphates",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alcohol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "quality",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "565db172-c73f-4510-989f-8598f66629c2",
       "rows": [
        [
         "0",
         "7.4",
         "0.7",
         "0.0",
         "1.9",
         "0.076",
         "11.0",
         "34.0",
         "0.9978",
         "3.51",
         "0.56",
         "9.4",
         "5"
        ],
        [
         "1",
         "7.8",
         "0.88",
         "0.0",
         "2.6",
         "0.098",
         "25.0",
         "67.0",
         "0.9968",
         "3.2",
         "0.68",
         "9.8",
         "5"
        ],
        [
         "2",
         "7.8",
         "0.76",
         "0.04",
         "2.3",
         "0.092",
         "15.0",
         "54.0",
         "0.997",
         "3.26",
         "0.65",
         "9.8",
         "5"
        ],
        [
         "3",
         "11.2",
         "0.28",
         "0.56",
         "1.9",
         "0.075",
         "17.0",
         "60.0",
         "0.998",
         "3.16",
         "0.58",
         "9.8",
         "6"
        ],
        [
         "4",
         "7.4",
         "0.7",
         "0.0",
         "1.9",
         "0.076",
         "11.0",
         "34.0",
         "0.9978",
         "3.51",
         "0.56",
         "9.4",
         "5"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar los datos con pandas\n",
    "df_red = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
    "    sep=';'\n",
    ")\n",
    "df_white = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',\n",
    "    sep=';'\n",
    ")\n",
    "df = pd.concat([df_red, df_white])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7c2f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', \n",
    "    'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'\n",
    "]\n",
    "\n",
    "\n",
    "# separar features y target\n",
    "y = df.pop('quality').values\n",
    "X = df.copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4adb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train, y_train shapes: (4872, 11) (4872,)\n",
      "x_test, y_test shapes: (1625, 11) (1625,)\n",
      "Some qualities:  [6 7 8 5 6]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
    "print('x_test, y_test shapes:', x_test.shape, y_test.shape)\n",
    "print('Some qualities: ', y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "painted-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Si quiere, puede normalizar las features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-planner",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## Cuestión 1: Cree un modelo secuencial que contenga 4 capas ocultas(hidden layers), con más de 60 neuronas  por capa, sin regularización y obtenga los resultados.\n",
    "\n",
    "Puntuación: \n",
    "- Obtener el modelo correcto: 0.8 pts\n",
    "- Compilar el modelo: 0.1pts\n",
    "- Acertar con la función de pérdida: 0.1 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "working-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "# Código aquí\n",
    "\n",
    "# Primera capa\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(11,), ))\n",
    "# Segunda capa\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "# Tercera capa\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "# Cuarta capa\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "mobile-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "# Código aquí\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "rotary-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4724 - loss: 1.2945 - val_accuracy: 0.5549 - val_loss: 1.0853\n",
      "Epoch 2/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5674 - loss: 1.0550 - val_accuracy: 0.5744 - val_loss: 1.0330\n",
      "Epoch 3/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5761 - loss: 0.9943 - val_accuracy: 0.5610 - val_loss: 1.0341\n",
      "Epoch 4/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5625 - loss: 1.0234 - val_accuracy: 0.5600 - val_loss: 1.0259\n",
      "Epoch 5/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5903 - loss: 0.9741 - val_accuracy: 0.5856 - val_loss: 1.0168\n",
      "Epoch 6/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5842 - loss: 0.9642 - val_accuracy: 0.5815 - val_loss: 1.0097\n",
      "Epoch 7/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6084 - loss: 0.9331 - val_accuracy: 0.5795 - val_loss: 1.0198\n",
      "Epoch 8/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6054 - loss: 0.9425 - val_accuracy: 0.5641 - val_loss: 1.0150\n",
      "Epoch 9/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6092 - loss: 0.9118 - val_accuracy: 0.5733 - val_loss: 1.0039\n",
      "Epoch 10/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6218 - loss: 0.8780 - val_accuracy: 0.5590 - val_loss: 1.0177\n",
      "Epoch 11/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6199 - loss: 0.8822 - val_accuracy: 0.5590 - val_loss: 1.0555\n",
      "Epoch 12/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6213 - loss: 0.8768 - val_accuracy: 0.5631 - val_loss: 1.0123\n",
      "Epoch 13/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6358 - loss: 0.8519 - val_accuracy: 0.5559 - val_loss: 1.0621\n",
      "Epoch 14/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6455 - loss: 0.8309 - val_accuracy: 0.5487 - val_loss: 1.0518\n",
      "Epoch 15/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6588 - loss: 0.7962 - val_accuracy: 0.5610 - val_loss: 1.0467\n",
      "Epoch 16/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6378 - loss: 0.7949 - val_accuracy: 0.5590 - val_loss: 1.0373\n",
      "Epoch 17/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6732 - loss: 0.7744 - val_accuracy: 0.5621 - val_loss: 1.0642\n",
      "Epoch 18/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6814 - loss: 0.7452 - val_accuracy: 0.5785 - val_loss: 1.0501\n",
      "Epoch 19/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6946 - loss: 0.7339 - val_accuracy: 0.5723 - val_loss: 1.0569\n",
      "Epoch 20/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6910 - loss: 0.7102 - val_accuracy: 0.5621 - val_loss: 1.0816\n",
      "Epoch 21/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7146 - loss: 0.6825 - val_accuracy: 0.5713 - val_loss: 1.0996\n",
      "Epoch 22/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7137 - loss: 0.6754 - val_accuracy: 0.5590 - val_loss: 1.0967\n",
      "Epoch 23/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7251 - loss: 0.6594 - val_accuracy: 0.5703 - val_loss: 1.1354\n",
      "Epoch 24/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7407 - loss: 0.6193 - val_accuracy: 0.5744 - val_loss: 1.1121\n",
      "Epoch 25/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7342 - loss: 0.6265 - val_accuracy: 0.5733 - val_loss: 1.1408\n",
      "Epoch 26/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7388 - loss: 0.6008 - val_accuracy: 0.5662 - val_loss: 1.2129\n",
      "Epoch 27/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7766 - loss: 0.5697 - val_accuracy: 0.5621 - val_loss: 1.2074\n",
      "Epoch 28/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7688 - loss: 0.5666 - val_accuracy: 0.5764 - val_loss: 1.2132\n",
      "Epoch 29/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.5598 - val_accuracy: 0.5764 - val_loss: 1.2605\n",
      "Epoch 30/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7973 - loss: 0.4980 - val_accuracy: 0.5774 - val_loss: 1.2640\n",
      "Epoch 31/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8085 - loss: 0.4846 - val_accuracy: 0.5744 - val_loss: 1.2796\n",
      "Epoch 32/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7974 - loss: 0.4941 - val_accuracy: 0.5846 - val_loss: 1.2801\n",
      "Epoch 33/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.4477 - val_accuracy: 0.5672 - val_loss: 1.4068\n",
      "Epoch 34/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8033 - loss: 0.4808 - val_accuracy: 0.5949 - val_loss: 1.3630\n",
      "Epoch 35/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8261 - loss: 0.4274 - val_accuracy: 0.5836 - val_loss: 1.3941\n",
      "Epoch 36/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8339 - loss: 0.4117 - val_accuracy: 0.5877 - val_loss: 1.4242\n",
      "Epoch 37/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8527 - loss: 0.3884 - val_accuracy: 0.5805 - val_loss: 1.4248\n",
      "Epoch 38/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8474 - loss: 0.3780 - val_accuracy: 0.6021 - val_loss: 1.4159\n",
      "Epoch 39/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8558 - loss: 0.3614 - val_accuracy: 0.5764 - val_loss: 1.5560\n",
      "Epoch 40/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8501 - loss: 0.3663 - val_accuracy: 0.6062 - val_loss: 1.5157\n",
      "Epoch 41/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.3393 - val_accuracy: 0.5897 - val_loss: 1.5270\n",
      "Epoch 42/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8804 - loss: 0.3005 - val_accuracy: 0.5785 - val_loss: 1.6215\n",
      "Epoch 43/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8811 - loss: 0.3086 - val_accuracy: 0.5590 - val_loss: 1.6211\n",
      "Epoch 44/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8854 - loss: 0.2931 - val_accuracy: 0.5713 - val_loss: 1.6233\n",
      "Epoch 45/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8815 - loss: 0.2958 - val_accuracy: 0.5877 - val_loss: 1.6734\n",
      "Epoch 46/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8877 - loss: 0.2902 - val_accuracy: 0.5815 - val_loss: 1.7261\n",
      "Epoch 47/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8901 - loss: 0.2752 - val_accuracy: 0.5938 - val_loss: 1.6958\n",
      "Epoch 48/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9034 - loss: 0.2506 - val_accuracy: 0.5703 - val_loss: 1.7564\n",
      "Epoch 49/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8997 - loss: 0.2497 - val_accuracy: 0.5897 - val_loss: 1.7615\n",
      "Epoch 50/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8997 - loss: 0.2530 - val_accuracy: 0.5887 - val_loss: 1.8300\n",
      "Epoch 51/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8901 - loss: 0.2649 - val_accuracy: 0.5785 - val_loss: 1.8529\n",
      "Epoch 52/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9077 - loss: 0.2420 - val_accuracy: 0.6133 - val_loss: 1.8706\n",
      "Epoch 53/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.2180 - val_accuracy: 0.5887 - val_loss: 1.9106\n",
      "Epoch 54/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9310 - loss: 0.1843 - val_accuracy: 0.5897 - val_loss: 1.9335\n",
      "Epoch 55/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1821 - val_accuracy: 0.5856 - val_loss: 1.9488\n",
      "Epoch 56/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9267 - loss: 0.1911 - val_accuracy: 0.6051 - val_loss: 1.9825\n",
      "Epoch 57/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9273 - loss: 0.1962 - val_accuracy: 0.5887 - val_loss: 2.0406\n",
      "Epoch 58/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9232 - loss: 0.1954 - val_accuracy: 0.6000 - val_loss: 2.0699\n",
      "Epoch 59/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9314 - loss: 0.1925 - val_accuracy: 0.5713 - val_loss: 2.1467\n",
      "Epoch 60/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9461 - loss: 0.1642 - val_accuracy: 0.5815 - val_loss: 2.1121\n",
      "Epoch 61/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9481 - loss: 0.1481 - val_accuracy: 0.5908 - val_loss: 2.1295\n",
      "Epoch 62/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9342 - loss: 0.1744 - val_accuracy: 0.5856 - val_loss: 2.2313\n",
      "Epoch 63/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9336 - loss: 0.1769 - val_accuracy: 0.5662 - val_loss: 2.2354\n",
      "Epoch 64/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1485 - val_accuracy: 0.5887 - val_loss: 2.2094\n",
      "Epoch 65/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9446 - loss: 0.1511 - val_accuracy: 0.6000 - val_loss: 2.2437\n",
      "Epoch 66/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9512 - loss: 0.1369 - val_accuracy: 0.5887 - val_loss: 2.2543\n",
      "Epoch 67/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9495 - loss: 0.1373 - val_accuracy: 0.5744 - val_loss: 2.3303\n",
      "Epoch 68/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9410 - loss: 0.1600 - val_accuracy: 0.5938 - val_loss: 2.2104\n",
      "Epoch 69/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9441 - loss: 0.1509 - val_accuracy: 0.5764 - val_loss: 2.3824\n",
      "Epoch 70/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9506 - loss: 0.1410 - val_accuracy: 0.5713 - val_loss: 2.3434\n",
      "Epoch 71/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1471 - val_accuracy: 0.5918 - val_loss: 2.3471\n",
      "Epoch 72/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9534 - loss: 0.1292 - val_accuracy: 0.6113 - val_loss: 2.3464\n",
      "Epoch 73/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9475 - loss: 0.1365 - val_accuracy: 0.5538 - val_loss: 2.4944\n",
      "Epoch 74/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9580 - loss: 0.1134 - val_accuracy: 0.5969 - val_loss: 2.3742\n",
      "Epoch 75/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9600 - loss: 0.1137 - val_accuracy: 0.5908 - val_loss: 2.4206\n",
      "Epoch 76/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9603 - loss: 0.1056 - val_accuracy: 0.5887 - val_loss: 2.4745\n",
      "Epoch 77/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1201 - val_accuracy: 0.5867 - val_loss: 2.5149\n",
      "Epoch 78/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9459 - loss: 0.1277 - val_accuracy: 0.5805 - val_loss: 2.5897\n",
      "Epoch 79/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1132 - val_accuracy: 0.6041 - val_loss: 2.5339\n",
      "Epoch 80/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.0966 - val_accuracy: 0.5897 - val_loss: 2.4895\n",
      "Epoch 81/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9714 - loss: 0.0877 - val_accuracy: 0.5979 - val_loss: 2.6287\n",
      "Epoch 82/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9567 - loss: 0.1168 - val_accuracy: 0.5938 - val_loss: 2.5869\n",
      "Epoch 83/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1198 - val_accuracy: 0.6051 - val_loss: 2.5173\n",
      "Epoch 84/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9644 - loss: 0.1066 - val_accuracy: 0.5744 - val_loss: 2.6480\n",
      "Epoch 85/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9670 - loss: 0.0983 - val_accuracy: 0.5672 - val_loss: 2.6995\n",
      "Epoch 86/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.0960 - val_accuracy: 0.5928 - val_loss: 2.7400\n",
      "Epoch 87/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9602 - loss: 0.1008 - val_accuracy: 0.6041 - val_loss: 2.5886\n",
      "Epoch 88/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9610 - loss: 0.1118 - val_accuracy: 0.5508 - val_loss: 2.7055\n",
      "Epoch 89/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.0912 - val_accuracy: 0.5785 - val_loss: 2.7039\n",
      "Epoch 90/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1045 - val_accuracy: 0.5928 - val_loss: 2.6535\n",
      "Epoch 91/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.0970 - val_accuracy: 0.6031 - val_loss: 2.6655\n",
      "Epoch 92/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9858 - loss: 0.0517 - val_accuracy: 0.5867 - val_loss: 2.7282\n",
      "Epoch 93/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.0840 - val_accuracy: 0.5990 - val_loss: 2.8895\n",
      "Epoch 94/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1091 - val_accuracy: 0.5877 - val_loss: 2.8486\n",
      "Epoch 95/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9538 - loss: 0.1240 - val_accuracy: 0.5979 - val_loss: 2.7093\n",
      "Epoch 96/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1134 - val_accuracy: 0.5867 - val_loss: 2.7108\n",
      "Epoch 97/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9606 - loss: 0.1071 - val_accuracy: 0.5836 - val_loss: 2.6418\n",
      "Epoch 98/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9778 - loss: 0.0653 - val_accuracy: 0.5979 - val_loss: 2.7440\n",
      "Epoch 99/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0595 - val_accuracy: 0.5908 - val_loss: 2.7927\n",
      "Epoch 100/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9855 - loss: 0.0479 - val_accuracy: 0.5979 - val_loss: 2.7781\n",
      "Epoch 101/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.0503 - val_accuracy: 0.5836 - val_loss: 2.8359\n",
      "Epoch 102/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9718 - loss: 0.0839 - val_accuracy: 0.6021 - val_loss: 2.9157\n",
      "Epoch 103/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1256 - val_accuracy: 0.6000 - val_loss: 2.8688\n",
      "Epoch 104/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9533 - loss: 0.1311 - val_accuracy: 0.5938 - val_loss: 2.7499\n",
      "Epoch 105/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1151 - val_accuracy: 0.5979 - val_loss: 2.7605\n",
      "Epoch 106/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0767 - val_accuracy: 0.5969 - val_loss: 2.8867\n",
      "Epoch 107/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9831 - loss: 0.0503 - val_accuracy: 0.5938 - val_loss: 2.7980\n",
      "Epoch 108/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0613 - val_accuracy: 0.5990 - val_loss: 2.8386\n",
      "Epoch 109/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9871 - loss: 0.0382 - val_accuracy: 0.5969 - val_loss: 2.8717\n",
      "Epoch 110/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9940 - loss: 0.0266 - val_accuracy: 0.5949 - val_loss: 2.9606\n",
      "Epoch 111/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9803 - loss: 0.0571 - val_accuracy: 0.5928 - val_loss: 2.9396\n",
      "Epoch 112/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.0620 - val_accuracy: 0.6010 - val_loss: 2.8965\n",
      "Epoch 113/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0539 - val_accuracy: 0.6082 - val_loss: 2.9528\n",
      "Epoch 114/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1026 - val_accuracy: 0.5836 - val_loss: 3.0603\n",
      "Epoch 115/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9448 - loss: 0.1447 - val_accuracy: 0.5846 - val_loss: 2.8483\n",
      "Epoch 116/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9499 - loss: 0.1326 - val_accuracy: 0.5836 - val_loss: 2.8564\n",
      "Epoch 117/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9737 - loss: 0.0691 - val_accuracy: 0.6031 - val_loss: 2.8559\n",
      "Epoch 118/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.0536 - val_accuracy: 0.5856 - val_loss: 2.8967\n",
      "Epoch 119/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0391 - val_accuracy: 0.5990 - val_loss: 2.8798\n",
      "Epoch 120/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0236 - val_accuracy: 0.5969 - val_loss: 2.9689\n",
      "Epoch 121/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0287 - val_accuracy: 0.5959 - val_loss: 3.0025\n",
      "Epoch 122/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0249 - val_accuracy: 0.6041 - val_loss: 2.9486\n",
      "Epoch 123/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0198 - val_accuracy: 0.5959 - val_loss: 3.0009\n",
      "Epoch 124/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0279 - val_accuracy: 0.5908 - val_loss: 3.0571\n",
      "Epoch 125/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9675 - loss: 0.0913 - val_accuracy: 0.5785 - val_loss: 3.0880\n",
      "Epoch 126/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1604 - val_accuracy: 0.5549 - val_loss: 2.9378\n",
      "Epoch 127/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9277 - loss: 0.2034 - val_accuracy: 0.5969 - val_loss: 2.7892\n",
      "Epoch 128/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0836 - val_accuracy: 0.6072 - val_loss: 2.8942\n",
      "Epoch 129/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0420 - val_accuracy: 0.5979 - val_loss: 2.9021\n",
      "Epoch 130/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0233 - val_accuracy: 0.6133 - val_loss: 2.9030\n",
      "Epoch 131/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0244 - val_accuracy: 0.6113 - val_loss: 2.9135\n",
      "Epoch 132/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0227 - val_accuracy: 0.6051 - val_loss: 2.9987\n",
      "Epoch 133/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0240 - val_accuracy: 0.6041 - val_loss: 3.0034\n",
      "Epoch 134/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0241 - val_accuracy: 0.6072 - val_loss: 3.0022\n",
      "Epoch 135/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0209 - val_accuracy: 0.6051 - val_loss: 3.0619\n",
      "Epoch 136/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0619 - val_accuracy: 0.5959 - val_loss: 3.1644\n",
      "Epoch 137/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.1430 - val_accuracy: 0.5569 - val_loss: 3.0657\n",
      "Epoch 138/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9330 - loss: 0.1825 - val_accuracy: 0.6041 - val_loss: 2.7508\n",
      "Epoch 139/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0817 - val_accuracy: 0.6092 - val_loss: 2.8112\n",
      "Epoch 140/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0314 - val_accuracy: 0.6082 - val_loss: 2.9063\n",
      "Epoch 141/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0235 - val_accuracy: 0.6041 - val_loss: 2.9860\n",
      "Epoch 142/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0162 - val_accuracy: 0.6113 - val_loss: 2.9753\n",
      "Epoch 143/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0150 - val_accuracy: 0.6031 - val_loss: 3.0563\n",
      "Epoch 144/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0171 - val_accuracy: 0.6031 - val_loss: 3.0887\n",
      "Epoch 145/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0207 - val_accuracy: 0.5938 - val_loss: 3.1581\n",
      "Epoch 146/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0279 - val_accuracy: 0.5959 - val_loss: 3.1956\n",
      "Epoch 147/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0516 - val_accuracy: 0.6010 - val_loss: 3.0507\n",
      "Epoch 148/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0693 - val_accuracy: 0.5856 - val_loss: 3.0689\n",
      "Epoch 149/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1838 - val_accuracy: 0.5928 - val_loss: 2.9430\n",
      "Epoch 150/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9483 - loss: 0.1589 - val_accuracy: 0.5969 - val_loss: 2.8032\n",
      "Epoch 151/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.0827 - val_accuracy: 0.6000 - val_loss: 2.8657\n",
      "Epoch 152/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0228 - val_accuracy: 0.5949 - val_loss: 2.9316\n",
      "Epoch 153/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0227 - val_accuracy: 0.6072 - val_loss: 2.9973\n",
      "Epoch 154/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0252 - val_accuracy: 0.6103 - val_loss: 3.0026\n",
      "Epoch 155/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0202 - val_accuracy: 0.5928 - val_loss: 3.0429\n",
      "Epoch 156/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0221 - val_accuracy: 0.5928 - val_loss: 3.0501\n",
      "Epoch 157/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0252 - val_accuracy: 0.6021 - val_loss: 3.0668\n",
      "Epoch 158/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0189 - val_accuracy: 0.6072 - val_loss: 3.0158\n",
      "Epoch 159/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0327 - val_accuracy: 0.6092 - val_loss: 3.0599\n",
      "Epoch 160/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0673 - val_accuracy: 0.5867 - val_loss: 3.1562\n",
      "Epoch 161/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9340 - loss: 0.1892 - val_accuracy: 0.5682 - val_loss: 3.0912\n",
      "Epoch 162/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9579 - loss: 0.1266 - val_accuracy: 0.5795 - val_loss: 3.1144\n",
      "Epoch 163/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0607 - val_accuracy: 0.6041 - val_loss: 3.0080\n",
      "Epoch 164/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0266 - val_accuracy: 0.5990 - val_loss: 2.9834\n",
      "Epoch 165/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0227 - val_accuracy: 0.6041 - val_loss: 3.0473\n",
      "Epoch 166/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0147 - val_accuracy: 0.6031 - val_loss: 3.0940\n",
      "Epoch 167/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0109 - val_accuracy: 0.6031 - val_loss: 3.1231\n",
      "Epoch 168/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0083 - val_accuracy: 0.6123 - val_loss: 3.1395\n",
      "Epoch 169/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9962 - loss: 0.0115 - val_accuracy: 0.5959 - val_loss: 3.1986\n",
      "Epoch 170/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0438 - val_accuracy: 0.5703 - val_loss: 3.2663\n",
      "Epoch 171/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.0808 - val_accuracy: 0.6021 - val_loss: 3.0675\n",
      "Epoch 172/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.1897 - val_accuracy: 0.5723 - val_loss: 2.9230\n",
      "Epoch 173/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.0888 - val_accuracy: 0.5938 - val_loss: 3.0439\n",
      "Epoch 174/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9873 - loss: 0.0483 - val_accuracy: 0.6123 - val_loss: 2.8602\n",
      "Epoch 175/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0353 - val_accuracy: 0.6051 - val_loss: 2.9246\n",
      "Epoch 176/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0211 - val_accuracy: 0.6041 - val_loss: 2.9954\n",
      "Epoch 177/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0098 - val_accuracy: 0.6010 - val_loss: 3.0210\n",
      "Epoch 178/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0100 - val_accuracy: 0.6144 - val_loss: 3.0466\n",
      "Epoch 179/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0114 - val_accuracy: 0.6000 - val_loss: 3.0704\n",
      "Epoch 180/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9917 - loss: 0.0229 - val_accuracy: 0.6010 - val_loss: 3.0679\n",
      "Epoch 181/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9939 - loss: 0.0171 - val_accuracy: 0.6021 - val_loss: 3.1210\n",
      "Epoch 182/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9951 - loss: 0.0160 - val_accuracy: 0.5908 - val_loss: 3.1883\n",
      "Epoch 183/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0179 - val_accuracy: 0.6000 - val_loss: 3.2561\n",
      "Epoch 184/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0230 - val_accuracy: 0.6000 - val_loss: 3.1790\n",
      "Epoch 185/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0196 - val_accuracy: 0.5979 - val_loss: 3.1631\n",
      "Epoch 186/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.0650 - val_accuracy: 0.5897 - val_loss: 3.3021\n",
      "Epoch 187/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9164 - loss: 0.2692 - val_accuracy: 0.5856 - val_loss: 2.9337\n",
      "Epoch 188/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9396 - loss: 0.1739 - val_accuracy: 0.5846 - val_loss: 2.8875\n",
      "Epoch 189/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0388 - val_accuracy: 0.6133 - val_loss: 2.8074\n",
      "Epoch 190/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0207 - val_accuracy: 0.6062 - val_loss: 2.8921\n",
      "Epoch 191/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0109 - val_accuracy: 0.6051 - val_loss: 2.9375\n",
      "Epoch 192/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0119 - val_accuracy: 0.6062 - val_loss: 2.9605\n",
      "Epoch 193/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9973 - loss: 0.0081 - val_accuracy: 0.6113 - val_loss: 3.0128\n",
      "Epoch 194/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0086 - val_accuracy: 0.6041 - val_loss: 3.0547\n",
      "Epoch 195/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0076 - val_accuracy: 0.6062 - val_loss: 3.0543\n",
      "Epoch 196/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9973 - loss: 0.0068 - val_accuracy: 0.6072 - val_loss: 3.0808\n",
      "Epoch 197/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9954 - loss: 0.0098 - val_accuracy: 0.6092 - val_loss: 3.1047\n",
      "Epoch 198/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0097 - val_accuracy: 0.6133 - val_loss: 3.1034\n",
      "Epoch 199/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0093 - val_accuracy: 0.6062 - val_loss: 3.1559\n",
      "Epoch 200/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0065 - val_accuracy: 0.5990 - val_loss: 3.1591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e1febb0790>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No modifique el código\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "descending-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6227 - loss: 3.1332\n",
      "Test Loss: [3.3006839752197266, 0.6030769348144531]\n"
     ]
    }
   ],
   "source": [
    "# No modifique el código\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-delivery",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## Cuestión 2: Utilice el mismo modelo de la cuestión anterior pero añadiendo al menos dos técnicas distinas de regularización. No es necesario reducir el test loss.\n",
    "\n",
    "Ejemplos de regularización: [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "hired-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhony\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "# Código aquí\n",
    "# Primera capa\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(11,), ))\n",
    "model.add(layers.Dropout(0.3))\n",
    "# Segunda capa\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5,l2=5e-4)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "# Tercera capa\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.BatchNormalization())\n",
    "# Cuarta capa\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "focal-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "# Código aquí\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "338f8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "prostate-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2888 - loss: 2.1722 - val_accuracy: 0.5385 - val_loss: 1.6280\n",
      "Epoch 2/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4826 - loss: 1.6034 - val_accuracy: 0.5487 - val_loss: 1.3904\n",
      "Epoch 3/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5086 - loss: 1.4025 - val_accuracy: 0.5477 - val_loss: 1.2479\n",
      "Epoch 4/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5454 - loss: 1.2621 - val_accuracy: 0.5528 - val_loss: 1.1899\n",
      "Epoch 5/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5671 - loss: 1.1897 - val_accuracy: 0.5538 - val_loss: 1.1579\n",
      "Epoch 6/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5384 - loss: 1.1764 - val_accuracy: 0.5764 - val_loss: 1.1386\n",
      "Epoch 7/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5634 - loss: 1.1584 - val_accuracy: 0.5579 - val_loss: 1.1220\n",
      "Epoch 8/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5647 - loss: 1.1282 - val_accuracy: 0.5641 - val_loss: 1.1135\n",
      "Epoch 9/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5457 - loss: 1.1249 - val_accuracy: 0.5538 - val_loss: 1.1133\n",
      "Epoch 10/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5467 - loss: 1.1292 - val_accuracy: 0.5651 - val_loss: 1.1125\n",
      "Epoch 11/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5652 - loss: 1.0940 - val_accuracy: 0.5579 - val_loss: 1.0975\n",
      "Epoch 12/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5699 - loss: 1.0892 - val_accuracy: 0.5785 - val_loss: 1.0813\n",
      "Epoch 13/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5873 - loss: 1.0897 - val_accuracy: 0.5672 - val_loss: 1.0890\n",
      "Epoch 14/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5810 - loss: 1.0614 - val_accuracy: 0.5744 - val_loss: 1.0805\n",
      "Epoch 15/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5804 - loss: 1.0631 - val_accuracy: 0.5815 - val_loss: 1.0838\n",
      "Epoch 16/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5788 - loss: 1.0639 - val_accuracy: 0.5621 - val_loss: 1.0800\n",
      "Epoch 17/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5889 - loss: 1.0528 - val_accuracy: 0.5641 - val_loss: 1.0848\n",
      "Epoch 18/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5910 - loss: 1.0520 - val_accuracy: 0.5733 - val_loss: 1.0679\n",
      "Epoch 19/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5902 - loss: 1.0132 - val_accuracy: 0.5785 - val_loss: 1.0811\n",
      "Epoch 20/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5814 - loss: 1.0360 - val_accuracy: 0.5744 - val_loss: 1.0702\n",
      "Epoch 21/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5717 - loss: 1.0430 - val_accuracy: 0.5877 - val_loss: 1.0648\n",
      "Epoch 22/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5976 - loss: 1.0058 - val_accuracy: 0.5713 - val_loss: 1.0612\n",
      "Epoch 23/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5928 - loss: 1.0090 - val_accuracy: 0.5662 - val_loss: 1.0568\n",
      "Epoch 24/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5923 - loss: 0.9954 - val_accuracy: 0.5815 - val_loss: 1.0549\n",
      "Epoch 25/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6050 - loss: 1.0061 - val_accuracy: 0.5733 - val_loss: 1.0577\n",
      "Epoch 26/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6136 - loss: 0.9832 - val_accuracy: 0.5733 - val_loss: 1.0660\n",
      "Epoch 27/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5894 - loss: 0.9892 - val_accuracy: 0.5785 - val_loss: 1.0581\n",
      "Epoch 28/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5850 - loss: 1.0130 - val_accuracy: 0.5713 - val_loss: 1.0579\n",
      "Epoch 29/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6039 - loss: 0.9911 - val_accuracy: 0.5785 - val_loss: 1.0533\n",
      "Epoch 30/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5968 - loss: 1.0006 - val_accuracy: 0.5846 - val_loss: 1.0517\n",
      "Epoch 31/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6018 - loss: 0.9862 - val_accuracy: 0.5815 - val_loss: 1.0482\n",
      "Epoch 32/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5975 - loss: 0.9703 - val_accuracy: 0.5579 - val_loss: 1.0680\n",
      "Epoch 33/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6024 - loss: 0.9836 - val_accuracy: 0.5733 - val_loss: 1.0604\n",
      "Epoch 34/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6134 - loss: 0.9469 - val_accuracy: 0.5877 - val_loss: 1.0570\n",
      "Epoch 35/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5840 - loss: 0.9965 - val_accuracy: 0.5703 - val_loss: 1.0510\n",
      "Epoch 36/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6117 - loss: 0.9771 - val_accuracy: 0.5723 - val_loss: 1.0531\n",
      "Epoch 37/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6138 - loss: 0.9481 - val_accuracy: 0.5610 - val_loss: 1.0590\n",
      "Epoch 38/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6164 - loss: 0.9403 - val_accuracy: 0.5856 - val_loss: 1.0549\n",
      "Epoch 39/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6134 - loss: 0.9535 - val_accuracy: 0.5692 - val_loss: 1.0516\n",
      "Epoch 40/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6146 - loss: 0.9431 - val_accuracy: 0.5867 - val_loss: 1.0643\n",
      "Epoch 41/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6192 - loss: 0.9561 - val_accuracy: 0.5826 - val_loss: 1.0486\n",
      "Epoch 42/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6250 - loss: 0.9136 - val_accuracy: 0.5764 - val_loss: 1.0601\n",
      "Epoch 43/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6228 - loss: 0.9316 - val_accuracy: 0.5764 - val_loss: 1.0629\n",
      "Epoch 44/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6283 - loss: 0.9250 - val_accuracy: 0.5723 - val_loss: 1.0538\n",
      "Epoch 45/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6366 - loss: 0.9184 - val_accuracy: 0.5723 - val_loss: 1.0494\n",
      "Epoch 46/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6171 - loss: 0.9214 - val_accuracy: 0.5805 - val_loss: 1.0646\n",
      "Epoch 47/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6395 - loss: 0.8952 - val_accuracy: 0.5600 - val_loss: 1.0532\n",
      "Epoch 48/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6210 - loss: 0.9075 - val_accuracy: 0.5713 - val_loss: 1.0585\n",
      "Epoch 49/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6288 - loss: 0.9184 - val_accuracy: 0.5621 - val_loss: 1.0700\n",
      "Epoch 50/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6429 - loss: 0.8803 - val_accuracy: 0.5723 - val_loss: 1.0759\n",
      "Epoch 51/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6368 - loss: 0.9095 - val_accuracy: 0.5836 - val_loss: 1.0695\n",
      "Epoch 52/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6167 - loss: 0.9097 - val_accuracy: 0.5764 - val_loss: 1.0663\n",
      "Epoch 53/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6384 - loss: 0.8876 - val_accuracy: 0.5672 - val_loss: 1.0622\n",
      "Epoch 54/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6503 - loss: 0.8825 - val_accuracy: 0.5897 - val_loss: 1.0530\n",
      "Epoch 55/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6425 - loss: 0.8964 - val_accuracy: 0.5846 - val_loss: 1.0580\n",
      "Epoch 56/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6303 - loss: 0.8974 - val_accuracy: 0.5856 - val_loss: 1.0586\n",
      "Epoch 57/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6502 - loss: 0.8774 - val_accuracy: 0.5754 - val_loss: 1.0498\n",
      "Epoch 58/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6385 - loss: 0.8915 - val_accuracy: 0.5774 - val_loss: 1.0642\n",
      "Epoch 59/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6559 - loss: 0.8589 - val_accuracy: 0.5908 - val_loss: 1.0539\n",
      "Epoch 60/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6358 - loss: 0.8762 - val_accuracy: 0.5785 - val_loss: 1.0633\n",
      "Epoch 61/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6406 - loss: 0.8816 - val_accuracy: 0.5867 - val_loss: 1.0576\n",
      "Epoch 62/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6566 - loss: 0.8734 - val_accuracy: 0.5815 - val_loss: 1.0687\n",
      "Epoch 63/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6483 - loss: 0.8727 - val_accuracy: 0.5877 - val_loss: 1.0689\n",
      "Epoch 64/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6646 - loss: 0.8379 - val_accuracy: 0.5815 - val_loss: 1.0611\n",
      "Epoch 65/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6597 - loss: 0.8572 - val_accuracy: 0.5723 - val_loss: 1.0768\n",
      "Epoch 66/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6541 - loss: 0.8740 - val_accuracy: 0.5867 - val_loss: 1.0521\n",
      "Epoch 67/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6485 - loss: 0.8513 - val_accuracy: 0.5826 - val_loss: 1.0655\n",
      "Epoch 68/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6609 - loss: 0.8478 - val_accuracy: 0.5928 - val_loss: 1.0775\n",
      "Epoch 69/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6569 - loss: 0.8559 - val_accuracy: 0.6010 - val_loss: 1.0828\n",
      "Epoch 70/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6476 - loss: 0.8623 - val_accuracy: 0.5928 - val_loss: 1.0759\n",
      "Epoch 71/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6697 - loss: 0.8388 - val_accuracy: 0.6000 - val_loss: 1.0920\n",
      "Epoch 72/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6596 - loss: 0.8525 - val_accuracy: 0.5897 - val_loss: 1.0741\n",
      "Epoch 73/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6678 - loss: 0.8219 - val_accuracy: 0.5928 - val_loss: 1.0938\n",
      "Epoch 74/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6578 - loss: 0.8493 - val_accuracy: 0.6072 - val_loss: 1.0726\n",
      "Epoch 75/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6733 - loss: 0.8385 - val_accuracy: 0.5990 - val_loss: 1.0737\n",
      "Epoch 76/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6666 - loss: 0.8277 - val_accuracy: 0.5867 - val_loss: 1.0811\n",
      "Epoch 77/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6796 - loss: 0.8114 - val_accuracy: 0.5703 - val_loss: 1.0759\n",
      "Epoch 78/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6756 - loss: 0.8230 - val_accuracy: 0.5795 - val_loss: 1.0779\n",
      "Epoch 79/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6739 - loss: 0.8325 - val_accuracy: 0.5826 - val_loss: 1.0900\n",
      "Epoch 80/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6696 - loss: 0.8251 - val_accuracy: 0.6031 - val_loss: 1.0913\n",
      "Epoch 81/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6680 - loss: 0.8283 - val_accuracy: 0.6123 - val_loss: 1.0944\n",
      "Epoch 82/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6657 - loss: 0.8253 - val_accuracy: 0.5846 - val_loss: 1.0995\n",
      "Epoch 83/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6912 - loss: 0.7947 - val_accuracy: 0.5959 - val_loss: 1.1018\n",
      "Epoch 84/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6755 - loss: 0.8219 - val_accuracy: 0.5764 - val_loss: 1.1095\n",
      "Epoch 85/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6755 - loss: 0.8218 - val_accuracy: 0.5887 - val_loss: 1.1017\n",
      "Epoch 86/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7046 - loss: 0.7976 - val_accuracy: 0.5908 - val_loss: 1.1146\n",
      "Epoch 87/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6850 - loss: 0.7923 - val_accuracy: 0.5959 - val_loss: 1.1165\n",
      "Epoch 88/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6898 - loss: 0.8048 - val_accuracy: 0.5856 - val_loss: 1.1110\n",
      "Epoch 89/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6913 - loss: 0.7886 - val_accuracy: 0.5897 - val_loss: 1.1190\n",
      "Epoch 90/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6752 - loss: 0.8122 - val_accuracy: 0.6092 - val_loss: 1.0971\n",
      "Epoch 91/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6828 - loss: 0.7945 - val_accuracy: 0.5918 - val_loss: 1.1037\n",
      "Epoch 92/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6771 - loss: 0.8129 - val_accuracy: 0.6072 - val_loss: 1.1014\n",
      "Epoch 93/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7005 - loss: 0.7892 - val_accuracy: 0.6041 - val_loss: 1.1071\n",
      "Epoch 94/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6965 - loss: 0.7788 - val_accuracy: 0.5928 - val_loss: 1.1112\n",
      "Epoch 95/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7058 - loss: 0.7729 - val_accuracy: 0.5908 - val_loss: 1.1152\n",
      "Epoch 96/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6988 - loss: 0.7791 - val_accuracy: 0.6062 - val_loss: 1.1053\n",
      "Epoch 97/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6829 - loss: 0.8075 - val_accuracy: 0.6000 - val_loss: 1.1069\n",
      "Epoch 98/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6929 - loss: 0.7882 - val_accuracy: 0.6051 - val_loss: 1.1138\n",
      "Epoch 99/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7062 - loss: 0.7625 - val_accuracy: 0.5928 - val_loss: 1.1215\n",
      "Epoch 100/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7135 - loss: 0.7636 - val_accuracy: 0.6021 - val_loss: 1.1459\n",
      "Epoch 101/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6855 - loss: 0.7814 - val_accuracy: 0.5938 - val_loss: 1.1426\n",
      "Epoch 102/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6974 - loss: 0.7865 - val_accuracy: 0.5969 - val_loss: 1.1388\n",
      "Epoch 103/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6936 - loss: 0.7919 - val_accuracy: 0.5918 - val_loss: 1.1277\n",
      "Epoch 104/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7233 - loss: 0.7393 - val_accuracy: 0.5969 - val_loss: 1.1188\n",
      "Epoch 105/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6928 - loss: 0.7776 - val_accuracy: 0.5918 - val_loss: 1.1271\n",
      "Epoch 106/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6951 - loss: 0.7836 - val_accuracy: 0.5836 - val_loss: 1.1347\n",
      "Epoch 107/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7175 - loss: 0.7607 - val_accuracy: 0.5938 - val_loss: 1.1340\n",
      "Epoch 108/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7176 - loss: 0.7412 - val_accuracy: 0.6021 - val_loss: 1.1330\n",
      "Epoch 109/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7171 - loss: 0.7468 - val_accuracy: 0.6072 - val_loss: 1.1225\n",
      "Epoch 110/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6940 - loss: 0.7954 - val_accuracy: 0.5959 - val_loss: 1.1351\n",
      "Epoch 111/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7095 - loss: 0.7585 - val_accuracy: 0.5928 - val_loss: 1.1330\n",
      "Epoch 112/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7073 - loss: 0.7568 - val_accuracy: 0.6144 - val_loss: 1.1462\n",
      "Epoch 113/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7000 - loss: 0.7673 - val_accuracy: 0.6062 - val_loss: 1.1421\n",
      "Epoch 114/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7089 - loss: 0.7493 - val_accuracy: 0.5918 - val_loss: 1.1433\n",
      "Epoch 115/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7302 - loss: 0.7304 - val_accuracy: 0.6031 - val_loss: 1.1473\n",
      "Epoch 116/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7126 - loss: 0.7583 - val_accuracy: 0.6082 - val_loss: 1.1366\n",
      "Epoch 117/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7163 - loss: 0.7455 - val_accuracy: 0.6072 - val_loss: 1.1456\n",
      "Epoch 118/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6893 - loss: 0.7826 - val_accuracy: 0.5949 - val_loss: 1.1435\n",
      "Epoch 119/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7052 - loss: 0.7456 - val_accuracy: 0.5949 - val_loss: 1.1665\n",
      "Epoch 120/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7244 - loss: 0.7170 - val_accuracy: 0.6144 - val_loss: 1.1222\n",
      "Epoch 121/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7085 - loss: 0.7543 - val_accuracy: 0.6072 - val_loss: 1.1552\n",
      "Epoch 122/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7292 - loss: 0.7309 - val_accuracy: 0.5949 - val_loss: 1.1700\n",
      "Epoch 123/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7125 - loss: 0.7357 - val_accuracy: 0.5990 - val_loss: 1.1487\n",
      "Epoch 124/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7146 - loss: 0.7504 - val_accuracy: 0.5938 - val_loss: 1.1540\n",
      "Epoch 125/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7134 - loss: 0.7358 - val_accuracy: 0.6062 - val_loss: 1.1520\n",
      "Epoch 126/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7220 - loss: 0.7483 - val_accuracy: 0.6051 - val_loss: 1.1448\n",
      "Epoch 127/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7398 - loss: 0.7168 - val_accuracy: 0.6041 - val_loss: 1.1357\n",
      "Epoch 128/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7161 - loss: 0.7472 - val_accuracy: 0.6092 - val_loss: 1.1570\n",
      "Epoch 129/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7105 - loss: 0.7563 - val_accuracy: 0.6021 - val_loss: 1.1593\n",
      "Epoch 130/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7262 - loss: 0.7344 - val_accuracy: 0.5928 - val_loss: 1.1733\n",
      "Epoch 131/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7047 - loss: 0.7469 - val_accuracy: 0.6031 - val_loss: 1.1622\n",
      "Epoch 132/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7372 - loss: 0.7053 - val_accuracy: 0.6010 - val_loss: 1.1831\n",
      "Epoch 133/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7228 - loss: 0.7460 - val_accuracy: 0.6000 - val_loss: 1.1842\n",
      "Epoch 134/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7299 - loss: 0.7197 - val_accuracy: 0.5949 - val_loss: 1.1969\n",
      "Epoch 135/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7047 - loss: 0.7597 - val_accuracy: 0.5938 - val_loss: 1.1742\n",
      "Epoch 136/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7171 - loss: 0.7242 - val_accuracy: 0.6123 - val_loss: 1.1853\n",
      "Epoch 137/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7238 - loss: 0.7394 - val_accuracy: 0.6031 - val_loss: 1.1743\n",
      "Epoch 138/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7049 - loss: 0.7563 - val_accuracy: 0.5949 - val_loss: 1.1742\n",
      "Epoch 139/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7294 - loss: 0.7242 - val_accuracy: 0.5969 - val_loss: 1.1669\n",
      "Epoch 140/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7163 - loss: 0.7309 - val_accuracy: 0.5959 - val_loss: 1.1809\n",
      "Epoch 141/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7140 - loss: 0.7582 - val_accuracy: 0.6010 - val_loss: 1.1732\n",
      "Epoch 142/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7174 - loss: 0.7415 - val_accuracy: 0.6051 - val_loss: 1.1813\n",
      "Epoch 143/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7238 - loss: 0.7225 - val_accuracy: 0.6133 - val_loss: 1.1635\n",
      "Epoch 144/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7316 - loss: 0.7103 - val_accuracy: 0.6103 - val_loss: 1.1849\n",
      "Epoch 145/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7262 - loss: 0.7378 - val_accuracy: 0.6082 - val_loss: 1.1666\n",
      "Epoch 146/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7219 - loss: 0.7428 - val_accuracy: 0.6154 - val_loss: 1.1791\n",
      "Epoch 147/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7350 - loss: 0.7012 - val_accuracy: 0.5959 - val_loss: 1.1841\n",
      "Epoch 148/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7186 - loss: 0.7323 - val_accuracy: 0.5928 - val_loss: 1.1625\n",
      "Epoch 149/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7323 - loss: 0.7070 - val_accuracy: 0.5949 - val_loss: 1.1923\n",
      "Epoch 150/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7369 - loss: 0.7159 - val_accuracy: 0.6287 - val_loss: 1.1490\n",
      "Epoch 151/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7429 - loss: 0.6889 - val_accuracy: 0.6082 - val_loss: 1.1800\n",
      "Epoch 152/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7228 - loss: 0.7271 - val_accuracy: 0.5969 - val_loss: 1.1801\n",
      "Epoch 153/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7135 - loss: 0.7131 - val_accuracy: 0.6144 - val_loss: 1.1608\n",
      "Epoch 154/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7414 - loss: 0.7017 - val_accuracy: 0.6072 - val_loss: 1.1916\n",
      "Epoch 155/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7442 - loss: 0.6879 - val_accuracy: 0.6174 - val_loss: 1.1771\n",
      "Epoch 156/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7355 - loss: 0.7157 - val_accuracy: 0.6174 - val_loss: 1.1649\n",
      "Epoch 157/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7312 - loss: 0.7142 - val_accuracy: 0.6072 - val_loss: 1.1865\n",
      "Epoch 158/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7243 - loss: 0.7348 - val_accuracy: 0.6103 - val_loss: 1.1693\n",
      "Epoch 159/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7295 - loss: 0.7205 - val_accuracy: 0.6195 - val_loss: 1.1683\n",
      "Epoch 160/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7272 - loss: 0.7181 - val_accuracy: 0.6236 - val_loss: 1.1640\n",
      "Epoch 161/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7485 - loss: 0.6970 - val_accuracy: 0.6226 - val_loss: 1.1790\n",
      "Epoch 162/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7378 - loss: 0.6937 - val_accuracy: 0.6185 - val_loss: 1.1683\n",
      "Epoch 163/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7315 - loss: 0.7116 - val_accuracy: 0.6205 - val_loss: 1.1450\n",
      "Epoch 164/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7344 - loss: 0.7125 - val_accuracy: 0.6113 - val_loss: 1.1667\n",
      "Epoch 165/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7328 - loss: 0.7151 - val_accuracy: 0.6185 - val_loss: 1.1826\n",
      "Epoch 166/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7301 - loss: 0.7032 - val_accuracy: 0.6349 - val_loss: 1.1961\n",
      "Epoch 167/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7372 - loss: 0.7027 - val_accuracy: 0.6092 - val_loss: 1.1935\n",
      "Epoch 168/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7306 - loss: 0.7159 - val_accuracy: 0.6123 - val_loss: 1.1833\n",
      "Epoch 169/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7444 - loss: 0.6949 - val_accuracy: 0.6000 - val_loss: 1.1938\n",
      "Epoch 170/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7419 - loss: 0.6999 - val_accuracy: 0.6205 - val_loss: 1.2014\n",
      "Epoch 171/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7337 - loss: 0.7132 - val_accuracy: 0.6236 - val_loss: 1.1863\n",
      "Epoch 172/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7464 - loss: 0.6808 - val_accuracy: 0.6000 - val_loss: 1.1927\n",
      "Epoch 173/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7420 - loss: 0.6888 - val_accuracy: 0.6308 - val_loss: 1.1851\n",
      "Epoch 174/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7501 - loss: 0.6908 - val_accuracy: 0.6297 - val_loss: 1.2016\n",
      "Epoch 175/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 0.6772 - val_accuracy: 0.6174 - val_loss: 1.2045\n",
      "Epoch 176/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7320 - loss: 0.7016 - val_accuracy: 0.6267 - val_loss: 1.1881\n",
      "Epoch 177/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7450 - loss: 0.6905 - val_accuracy: 0.6113 - val_loss: 1.1949\n",
      "Epoch 178/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7480 - loss: 0.6963 - val_accuracy: 0.6185 - val_loss: 1.2069\n",
      "Epoch 179/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7405 - loss: 0.6895 - val_accuracy: 0.6000 - val_loss: 1.2048\n",
      "Epoch 180/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7384 - loss: 0.7090 - val_accuracy: 0.6072 - val_loss: 1.2095\n",
      "Epoch 181/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7580 - loss: 0.6812 - val_accuracy: 0.6000 - val_loss: 1.2030\n",
      "Epoch 182/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7341 - loss: 0.7228 - val_accuracy: 0.6051 - val_loss: 1.2064\n",
      "Epoch 183/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7351 - loss: 0.6866 - val_accuracy: 0.6103 - val_loss: 1.1958\n",
      "Epoch 184/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7350 - loss: 0.7190 - val_accuracy: 0.6010 - val_loss: 1.2120\n",
      "Epoch 185/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7484 - loss: 0.6678 - val_accuracy: 0.6226 - val_loss: 1.2018\n",
      "Epoch 186/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7434 - loss: 0.7011 - val_accuracy: 0.6287 - val_loss: 1.1972\n",
      "Epoch 187/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.6916 - val_accuracy: 0.6144 - val_loss: 1.2205\n",
      "Epoch 188/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7294 - loss: 0.7072 - val_accuracy: 0.6277 - val_loss: 1.1928\n",
      "Epoch 189/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7602 - loss: 0.6678 - val_accuracy: 0.6287 - val_loss: 1.1938\n",
      "Epoch 190/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.6864 - val_accuracy: 0.6226 - val_loss: 1.2111\n",
      "Epoch 191/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7537 - loss: 0.6840 - val_accuracy: 0.6195 - val_loss: 1.2230\n",
      "Epoch 192/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7365 - loss: 0.6840 - val_accuracy: 0.6308 - val_loss: 1.2211\n",
      "Epoch 193/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7423 - loss: 0.6966 - val_accuracy: 0.6062 - val_loss: 1.2528\n",
      "Epoch 194/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7298 - loss: 0.7099 - val_accuracy: 0.6113 - val_loss: 1.2267\n",
      "Epoch 195/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7568 - loss: 0.6805 - val_accuracy: 0.6113 - val_loss: 1.2283\n",
      "Epoch 196/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7593 - loss: 0.6764 - val_accuracy: 0.6205 - val_loss: 1.2270\n",
      "Epoch 197/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7548 - loss: 0.6866 - val_accuracy: 0.6297 - val_loss: 1.2032\n",
      "Epoch 198/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7477 - loss: 0.6936 - val_accuracy: 0.6174 - val_loss: 1.2084\n",
      "Epoch 199/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7294 - loss: 0.6998 - val_accuracy: 0.5938 - val_loss: 1.2211\n",
      "Epoch 200/200\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7628 - loss: 0.6865 - val_accuracy: 0.6205 - val_loss: 1.1881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e18107a940>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No modifique el código\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "friendly-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5873 - loss: 1.1760\n",
      "Test Loss: [1.2536938190460205, 0.6000000238418579]\n"
     ]
    }
   ],
   "source": [
    "# No modifique el código\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-vegetation",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## Cuestión 3: Utilice el mismo modelo de la cuestión anterior pero añadiendo un callback de early stopping. No es necesario reducir el test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "precise-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhony\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "# Código aquí\n",
    "# Primera capa\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(11,), ))\n",
    "model.add(layers.Dropout(0.3))\n",
    "# Segunda capa\n",
    "model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5,l2=5e-4)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "# Tercera capa\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.BatchNormalization())\n",
    "# Cuarta capa\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "blond-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "# Código aquí\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "subsequent-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3412 - loss: 2.0837 - val_accuracy: 0.5333 - val_loss: 1.4396\n",
      "Epoch 2/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5100 - loss: 1.4414 - val_accuracy: 0.5446 - val_loss: 1.2166\n",
      "Epoch 3/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5236 - loss: 1.2574 - val_accuracy: 0.5467 - val_loss: 1.1681\n",
      "Epoch 4/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5172 - loss: 1.2060 - val_accuracy: 0.5395 - val_loss: 1.1518\n",
      "Epoch 5/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5718 - loss: 1.1262 - val_accuracy: 0.5354 - val_loss: 1.1510\n",
      "Epoch 6/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5393 - loss: 1.1564 - val_accuracy: 0.5538 - val_loss: 1.1328\n",
      "Epoch 7/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5525 - loss: 1.1253 - val_accuracy: 0.5559 - val_loss: 1.1245\n",
      "Epoch 8/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5697 - loss: 1.1227 - val_accuracy: 0.5549 - val_loss: 1.1152\n",
      "Epoch 9/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5504 - loss: 1.1299 - val_accuracy: 0.5610 - val_loss: 1.1000\n",
      "Epoch 10/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5524 - loss: 1.0901 - val_accuracy: 0.5590 - val_loss: 1.1127\n",
      "Epoch 11/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5684 - loss: 1.0991 - val_accuracy: 0.5703 - val_loss: 1.0932\n",
      "Epoch 12/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5560 - loss: 1.1031 - val_accuracy: 0.5497 - val_loss: 1.0965\n",
      "Epoch 13/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5631 - loss: 1.0910 - val_accuracy: 0.5744 - val_loss: 1.0795\n",
      "Epoch 14/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5732 - loss: 1.0662 - val_accuracy: 0.5733 - val_loss: 1.0803\n",
      "Epoch 15/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5761 - loss: 1.0634 - val_accuracy: 0.5805 - val_loss: 1.0752\n",
      "Epoch 16/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5796 - loss: 1.0658 - val_accuracy: 0.5682 - val_loss: 1.0697\n",
      "Epoch 17/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5780 - loss: 1.0528 - val_accuracy: 0.5785 - val_loss: 1.0675\n",
      "Epoch 18/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5711 - loss: 1.0560 - val_accuracy: 0.5631 - val_loss: 1.0681\n",
      "Epoch 19/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5981 - loss: 1.0254 - val_accuracy: 0.5467 - val_loss: 1.0838\n",
      "Epoch 20/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5700 - loss: 1.0494 - val_accuracy: 0.5815 - val_loss: 1.0613\n",
      "Epoch 21/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5654 - loss: 1.0542 - val_accuracy: 0.5569 - val_loss: 1.0651\n",
      "Epoch 22/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5693 - loss: 1.0369 - val_accuracy: 0.5631 - val_loss: 1.0612\n",
      "Epoch 23/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6080 - loss: 0.9855 - val_accuracy: 0.5590 - val_loss: 1.0650\n",
      "Epoch 24/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5900 - loss: 1.0037 - val_accuracy: 0.5723 - val_loss: 1.0611\n",
      "Epoch 25/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5933 - loss: 1.0030 - val_accuracy: 0.5692 - val_loss: 1.0502\n",
      "Epoch 26/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5906 - loss: 0.9978 - val_accuracy: 0.5662 - val_loss: 1.0538\n",
      "Epoch 27/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5960 - loss: 0.9810 - val_accuracy: 0.5774 - val_loss: 1.0527\n",
      "Epoch 28/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6074 - loss: 0.9825 - val_accuracy: 0.5713 - val_loss: 1.0635\n",
      "Epoch 29/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5948 - loss: 0.9779 - val_accuracy: 0.5692 - val_loss: 1.0573\n",
      "Epoch 30/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5991 - loss: 0.9676 - val_accuracy: 0.5928 - val_loss: 1.0453\n",
      "Epoch 31/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6036 - loss: 0.9705 - val_accuracy: 0.5600 - val_loss: 1.0543\n",
      "Epoch 32/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5965 - loss: 0.9800 - val_accuracy: 0.5733 - val_loss: 1.0555\n",
      "Epoch 33/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6115 - loss: 0.9591 - val_accuracy: 0.5805 - val_loss: 1.0595\n",
      "Epoch 34/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6147 - loss: 0.9612 - val_accuracy: 0.5549 - val_loss: 1.0657\n",
      "Epoch 35/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5981 - loss: 0.9806 - val_accuracy: 0.5723 - val_loss: 1.0547\n",
      "Epoch 36/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6101 - loss: 0.9564 - val_accuracy: 0.5621 - val_loss: 1.0612\n",
      "Epoch 37/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6098 - loss: 0.9665 - val_accuracy: 0.5815 - val_loss: 1.0588\n",
      "Epoch 38/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6199 - loss: 0.9626 - val_accuracy: 0.5733 - val_loss: 1.0531\n",
      "Epoch 39/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6006 - loss: 0.9379 - val_accuracy: 0.5815 - val_loss: 1.0465\n",
      "Epoch 40/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6324 - loss: 0.9468 - val_accuracy: 0.5908 - val_loss: 1.0653\n",
      "Epoch 41/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6050 - loss: 0.9571 - val_accuracy: 0.5723 - val_loss: 1.0498\n",
      "Epoch 42/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6116 - loss: 0.9609 - val_accuracy: 0.5703 - val_loss: 1.0626\n",
      "Epoch 43/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6102 - loss: 0.9355 - val_accuracy: 0.5610 - val_loss: 1.0588\n",
      "Epoch 44/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6199 - loss: 0.9234 - val_accuracy: 0.5764 - val_loss: 1.0685\n",
      "Epoch 45/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6238 - loss: 0.9126 - val_accuracy: 0.5641 - val_loss: 1.0712\n",
      "Epoch 46/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6222 - loss: 0.9317 - val_accuracy: 0.5621 - val_loss: 1.0656\n",
      "Epoch 47/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6038 - loss: 0.9402 - val_accuracy: 0.5744 - val_loss: 1.0549\n",
      "Epoch 48/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6342 - loss: 0.9227 - val_accuracy: 0.5703 - val_loss: 1.0567\n",
      "Epoch 49/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6316 - loss: 0.9342 - val_accuracy: 0.5662 - val_loss: 1.0759\n",
      "Epoch 50/200\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6160 - loss: 0.9187 - val_accuracy: 0.5641 - val_loss: 1.0796\n",
      "Epoch 50: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e1878fa760>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## definir el early stopping callback\n",
    "# Código aquí\n",
    "import datetime\n",
    "\n",
    "log_dir = 'log/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          verbose=1,\n",
    "          callbacks=[tensorboard_callback, es_callback]) # Código aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "pressing-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5712 - loss: 1.0578\n",
      "Test Loss: [1.0869908332824707, 0.5612307786941528]\n"
     ]
    }
   ],
   "source": [
    "# No modifique el código\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-lesbian",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## Cuestión 4: ¿Podría haberse usado otra función de activación de la neurona de salida? En caso afirmativo especifíquela."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-silicon",
   "metadata": {},
   "source": [
    "- Respuesta: \n",
    "\n",
    "    - No. La función de activación adecuada para este problema de clasificación es **Softmax** ya que convierte la salida en probabilidades que suman 1 para cada una de las 7 clases, facilitando así la clasificación exclusiva\n",
    "    - En el caso de la función de activación 'sigmoid', funcionaría siempre y cuando el problema fuese multilabel(clases independientes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-christianity",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "## Cuestión 5:  ¿Qué es lo que una neurona calcula?\n",
    "\n",
    "**a)** Una función de activación seguida de una suma ponderada  de las entradas.\n",
    "\n",
    "**b)** Una suma ponderada  de las entradas seguida de una función de activación.\n",
    "\n",
    "**c)** Una función de pérdida, definida sobre el target.\n",
    "\n",
    "**d)** Ninguna  de las anteriores es correcta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-burden",
   "metadata": {},
   "source": [
    "- **Respuesta**:\n",
    "    - b) Una suma ponderada de las entradas seguida de una función de activación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-european",
   "metadata": {},
   "source": [
    "<a name='1.6'></a>\n",
    "## Cuestión 6:  ¿Cuál de estas funciones de activación no debería usarse en una capa oculta (hidden layer)?\n",
    "\n",
    "**a)** `sigmoid`\n",
    "\n",
    "**b)** `tanh`\n",
    "\n",
    "**c)** `relu`\n",
    "\n",
    "**d)** `linear`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-attack",
   "metadata": {},
   "source": [
    "- **Respuesta**: \n",
    "    - d) linear. Porque es como si no 'hiciese nada'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-utilization",
   "metadata": {},
   "source": [
    "<a name='1.7'></a>\n",
    "## Cuestión 7:  ¿Cuál de estas técnicas es efectiva para combatir el overfitting en una red con varias capas ocultas? Ponga todas las que lo sean.\n",
    "\n",
    "**a)** Dropout\n",
    "\n",
    "**b)** Regularización L2.\n",
    "\n",
    "**c)** Aumentar el tamaño del test set.\n",
    "\n",
    "**d)** Aumentar el tamaño del validation set.\n",
    "\n",
    "**e)** Reducir el número de capas de la red.\n",
    "\n",
    "**f)** Data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-trainer",
   "metadata": {},
   "source": [
    "- **Respuestas**: \n",
    "    - a) Dropout\n",
    "    - b) Regularizaicón L2\n",
    "    - f) Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-deposit",
   "metadata": {},
   "source": [
    "<a name='1.8'></a>\n",
    "## Cuestión 8:  Supongamos que queremos entrenar una red para un problema de clasificación de imágenes con las siguientes clases: {'perro','gato','persona'}. ¿Cuántas neuronas y que función de activación debería tener la capa de salida? ¿Qué función de pérdida (loss function) debería usarse?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-roulette",
   "metadata": {},
   "source": [
    "- Respuestas:\n",
    "    - La cantidad de neuronas en la capa de salida deberían ser **3** (una por clase).\n",
    "    - La función de activación de la capa de salida debería ser **softmax**, ya que estamos tratando con una clasificación multiclase.\n",
    "    - La función de pérdida (loss function) debería ser **categorical_crossentropy**, ya que estamos tratando con una clasificación multiclase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
